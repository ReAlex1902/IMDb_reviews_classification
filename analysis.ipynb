{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "463krMYXmGI2",
        "colab_type": "text"
      },
      "source": [
        "#Reading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bZQ96KzmD7K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "\n",
        "SEED = 11\n",
        "\n",
        "torch.manual_seed(SEED)                         ## Reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "TEXT = data.Field(tokenize = 'spacy', include_lengths = True)   ## Text field\n",
        "LABEL = data.LabelField(dtype = torch.float)                    ## Label Field"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LD2-E1NbmtQL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "54035543-4dd6-4c18-b5b8-db6751d72beb"
      },
      "source": [
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:07<00:00, 11.3MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5LLD_ECnGUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "test_data, valid_data = test_data.split(random_state = random.seed(SEED))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyi_ZtuWnU5S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "16b39d7c-2c0d-4702-d671-e2d667dd1f6c"
      },
      "source": [
        "print(len(train_data), len(valid_data), len(test_data))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000 7500 17500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQtl1jCrnaao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Let's create 60 000 length vocabulary\n",
        "MAX_VOCAB_SIZE = 60000\n",
        "\n",
        "TEXT.build_vocab(train_data, \n",
        "                 max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = \"glove.6B.100d\", ## Global Vectors for Word Representation with 6B tokens and 100d\n",
        "                 unk_init = torch.Tensor.normal_) ## normal distribution for out-of-vocab words\n",
        "\n",
        "## uncomment the script bellow and comment the script above to read the saved vocabulary vocab.txt\n",
        "\n",
        "# import pickle\n",
        "# with open('vocab.txt', 'rb') as file:\n",
        "#     vocab = pickle.load(file)\n",
        "# TEXT.vocab = vocab\n",
        "\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9vzM-fqp4j8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "18315ba5-09b8-4a5f-d11c-46508548337d"
      },
      "source": [
        "print(f\"Number of words in TEXT vocab: {len(TEXT.vocab)}\")\n",
        "print(f\"Number of words in LABEL vocab: {len(LABEL.vocab)}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words in TEXT vocab: 60002\n",
            "Number of words in LABEL vocab: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR4_dpUIsYsK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "69b194b3-2913-4740-fcb6-cfeeae11e48a"
      },
      "source": [
        "print(TEXT.vocab.freqs.most_common(10))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('the', 289838), (',', 275296), ('.', 236843), ('and', 156483), ('a', 156282), ('of', 144055), ('to', 133886), ('is', 109095), ('in', 87676), ('I', 77546)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqBrF9_dv58r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')   ## Let's use GPU if abailable\n",
        "\n",
        "## BucketIterator will help us to minimize padding the amount of padding per batch\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_within_batch = True,\n",
        "    device = device)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67Q68ITyw07X",
        "colab_type": "text"
      },
      "source": [
        "#Creating the LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpX-0pcawzxY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
        "                 n_layers, bidirectional, dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers = n_layers, \n",
        "                           bidirectional = bidirectional, \n",
        "                           dropout = dropout)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        embedding = self.embedding(text)    ## shape = (sent_length, batch_size)\n",
        "        embedded = self.dropout(embedding)  ## shape = (sent_length, batch_size, emb_dim)\n",
        "        \n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)    ## pack sequence\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)        ## unpack sequence\n",
        "\n",
        "        ## output shape = (sent_len, batch_size, hid_dim * num_directions)\n",
        "        ## output over padding tokens are zero tensors\n",
        "        \n",
        "        ## hidden shape = (num_layers * num_directions, batch_size, hid_dim)\n",
        "        ## cell shape = (num_layers * num_directions, batch_size, hid_dim)\n",
        "        \n",
        "        ## concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
        "        ## and apply dropout\n",
        "        \n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)) ## shape = (batch_size, hid_dim * num_directions)\n",
        "            \n",
        "        return self.fc(hidden)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dMUBwvMwyet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.4\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model = Model(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePzI3mpl1j8J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e9a531a2-4ab9-4d93-8269-82f5d895b8ef"
      },
      "source": [
        "train_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"There are {train_params} trainable parameters\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 8310857 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjBvCn3J2ZaZ",
        "colab_type": "text"
      },
      "source": [
        "###Replace initial embedding with pretrained embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxYEB7OV16Z7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "8d6c8526-86a4-4f1d-ef81-0067eb8e0d12"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.5108,  1.0283, -0.3532,  ..., -0.3283,  1.5016, -0.0413],\n",
              "        [-1.7518, -2.9519,  0.0745,  ..., -1.8542,  0.2554, -0.1562],\n",
              "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
              "        ...,\n",
              "        [-0.5132,  0.2418, -0.7890,  ...,  0.2582, -0.3415,  0.1267],\n",
              "        [ 0.3138,  0.3586, -0.0497,  ...,  0.6948, -0.4473, -0.0625],\n",
              "        [-0.3410,  1.2659, -0.7284,  ...,  0.1234,  0.9430,  0.2376]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiZf4stM4Gdx",
        "colab_type": "text"
      },
      "source": [
        "### Replace <unk> and <pad> with zeros (they were initialized with the normal distribution)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZY5W7nxk2jwY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "3db08a10-35b7-4ac0-bd15-ebf3a8bd12fc"
      },
      "source": [
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "print(model.embedding.weight.data)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
            "        ...,\n",
            "        [-0.5132,  0.2418, -0.7890,  ...,  0.2582, -0.3415,  0.1267],\n",
            "        [ 0.3138,  0.3586, -0.0497,  ...,  0.6948, -0.4473, -0.0625],\n",
            "        [-0.3410,  1.2659, -0.7284,  ...,  0.1234,  0.9430,  0.2376]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piP5ieWk6WoC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "    \n",
        "    model.train()\n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        text, text_lengths = batch.text\n",
        "        predictions = model(text, text_lengths).squeeze(1)\n",
        "\n",
        "        loss = criterion(predictions, batch.label)\n",
        "        accuracy = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_accuracy += accuracy.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_accuracy / len(iterator)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu5FP76i6m21",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    accuracy = correct.sum() / len(correct)\n",
        "    return accuracy\n",
        "    \n",
        "def binary_classification_metrics(prediction, ground_truth):\n",
        "    '''\n",
        "    Computes metrics for binary classification\n",
        "\n",
        "    Arguments:\n",
        "    prediction, np array of bool (num_samples) - model predictions\n",
        "    ground_truth, np array of bool (num_samples) - true labels\n",
        "\n",
        "    Returns:\n",
        "    precision, recall, f1, accuracy - classification metrics\n",
        "    '''\n",
        "\n",
        "    prediction = torch.round(torch.sigmoid(prediction))\n",
        "    correct = (prediction == ground_truth).float() #convert into float for division \n",
        "    \n",
        "    precision = 0\n",
        "    recall = 0\n",
        "    accuracy = 0\n",
        "    f1 = 0\n",
        "\n",
        "    tp = 0      ## true positive\n",
        "    tn = 0      ## true negative\n",
        "    fp = 0      ## false positive\n",
        "    fn = 0      ## false negative\n",
        "\n",
        "    for i in range(len(prediction)):\n",
        "        if prediction[i] == True and ground_truth[i] == True:\n",
        "            tp += 1\n",
        "        if prediction[i] == True and ground_truth[i] == False:\n",
        "            fp += 1\n",
        "        if prediction[i] == False and ground_truth[i] == True:\n",
        "            fn += 1\n",
        "        if prediction[i] == False and ground_truth[i] == False:\n",
        "            tn += 1\n",
        "\n",
        "    accuracy = (tp + tn)/(tp + tn + fp + fn)\n",
        "    precision = tp/(tp + fp)\n",
        "    recall = tp/(tp + fn)\n",
        "    f1 = 2 * (precision * recall)/(precision + recall)\n",
        "\n",
        "    return precision, recall, f1, accuracy"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxsVXxSW7mmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "\n",
        "            text, text_lengths = batch.text\n",
        "            predictions = model(text, text_lengths).squeeze(1)\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            accuracy = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_accuracy += accuracy.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_accuracy / len(iterator)\n",
        "\n",
        "def metrics(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_f1 = 0\n",
        "\n",
        "    tp = tn = fp = fn = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "\n",
        "            text, text_lengths = batch.text\n",
        "            predictions = model(text, text_lengths).squeeze(1)\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            precision, recall, f1, accuracy = binary_classification_metrics(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_f1 += f1\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_f1 / len(iterator)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzi7HNrp719Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ6t2CNi8F2m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.0017)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQOZnPnT8Fks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = model.to(device)            ## use GPU\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrwCAcfC76ky",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "outputId": "19116386-df16-4113-e177-ae50508209b1"
      },
      "source": [
        "N_EPOCHS = 6\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_accuracy = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_accuracy = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model, 'model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_accuracy*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_accuracy*100:.2f}%')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type Model. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 2m 17s\n",
            "\tTrain Loss: 0.640 | Train Acc: 62.64%\n",
            "\t Val. Loss: 0.647 |  Val. Acc: 62.20%\n",
            "Epoch: 02 | Epoch Time: 2m 17s\n",
            "\tTrain Loss: 0.467 | Train Acc: 78.19%\n",
            "\t Val. Loss: 0.378 |  Val. Acc: 84.94%\n",
            "Epoch: 03 | Epoch Time: 2m 18s\n",
            "\tTrain Loss: 0.269 | Train Acc: 89.52%\n",
            "\t Val. Loss: 0.279 |  Val. Acc: 88.47%\n",
            "Epoch: 04 | Epoch Time: 2m 18s\n",
            "\tTrain Loss: 0.182 | Train Acc: 93.37%\n",
            "\t Val. Loss: 0.335 |  Val. Acc: 87.46%\n",
            "Epoch: 05 | Epoch Time: 2m 18s\n",
            "\tTrain Loss: 0.126 | Train Acc: 95.53%\n",
            "\t Val. Loss: 0.342 |  Val. Acc: 88.10%\n",
            "Epoch: 06 | Epoch Time: 2m 18s\n",
            "\tTrain Loss: 0.098 | Train Acc: 96.78%\n",
            "\t Val. Loss: 0.360 |  Val. Acc: 88.52%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4i1AQrv8SEO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "263fa986-9d8d-475d-b984-f9dd736ce2be"
      },
      "source": [
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.359 | Test Acc: 88.23%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJlZRwg6463n",
        "colab_type": "text"
      },
      "source": [
        "# Saving model and vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6CCCj_H433u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Use if you don't save your model during training\n",
        "# torch.save(model, 'model.pt')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7QqS-ll5CSq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "0053cc31-0700-4c9d-bbac-3d0d792a1805"
      },
      "source": [
        "def save_vocab(vocab, path):\n",
        "    import pickle\n",
        "    output = open(path, 'wb')\n",
        "    pickle.dump(vocab, output)\n",
        "    output.close()\n",
        "\n",
        "save_vocab(TEXT.vocab, 'vocab.txt')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
            "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eppVsZS15M51",
        "colab_type": "text"
      },
      "source": [
        "# Loading model and using for typical review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xsg_6RjP5bco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open('vocab.txt', 'rb') as file:\n",
        "    vocab = pickle.load(file)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIP80Bth5Ja7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def predict_sentiment(model, sentence):\n",
        "    model.eval()\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "    indexed = [vocab.stoi[t] for t in tokenized]\n",
        "    length = [len(indexed)]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
        "    return prediction.item()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5pJDtsw5WFa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b4ff691d-9d87-4b3e-8bb7-5746174d9ed3"
      },
      "source": [
        "sentence = \"Best movie ever\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "loaded_model = torch.load('model.pt', map_location = device)\n",
        "\n",
        "predict_sentiment(loaded_model, sentence)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8374526500701904"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W48LeTUrZazQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "849dd9d2-a86d-4aab-a960-384a05fa32fe"
      },
      "source": [
        "test_loss, test_f1 = metrics(loaded_model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test F1: {test_f1*100:.2f}%')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.274 | Test F1: 88.17%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBGcYNUM5uIB",
        "colab_type": "text"
      },
      "source": [
        "# Additional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-PCZLqjpBdM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "caabfd84-4549-4770-b765-0e8ee193a4ac"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('model.pt') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_55d5ac45-2779-4cc2-96f7-925492eb693b\", \"model.pt\", 33268914)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di7G0NEPrZum",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}